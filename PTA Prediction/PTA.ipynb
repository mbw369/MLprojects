{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_excel(\"PTA.xlsx\")\n",
    "pd.set_option(\"max_columns\", 9999)\n",
    "rand_state=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the 1 missing age row:\n",
    "\n",
    "data['Age'] = data['Age'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert continuous age variable to categorical:\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    if row['Age'] < 18:\n",
    "        data.loc[idx, 'Age_cat'] = \"Teenager\"\n",
    "    elif row['Age'] >= 18 and row['Age'] < 30:\n",
    "        data.loc[idx, 'Age_cat'] = \"Young Adult\"\n",
    "    elif row['Age'] >= 30 and row['Age'] < 50:\n",
    "        data.loc[idx, 'Age_cat'] = \"Adult\"\n",
    "    elif row['Age'] >50:\n",
    "        data.loc[idx, 'Age_cat'] = \"50+\"\n",
    "\n",
    "def create_dummies(df,column_name):\n",
    "    \n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df\n",
    "\n",
    "data = create_dummies(data, 'Age_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert gender to numerical:\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['Gender'] == \"M\":\n",
    "        data.loc[index,'Gender'] = 1\n",
    "    else:\n",
    "        data.loc[index,'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NR (null) value from Duration of symptoms:\n",
    "\n",
    "data = data [data['Duration Sxs (days)']!= \"NR\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deal with missing values for WBC count:\n",
    "\n",
    "import numpy as np\n",
    "for idx,row in data.iterrows():\n",
    "    if row['WBC '] == 'Not Performed':\n",
    "        data.loc[idx, 'WBC ']=np.nan\n",
    "    elif row['WBC '] == 0:\n",
    "        data.loc[idx, 'WBC ']=np.nan\n",
    "        \n",
    "data['WBC '] = data['WBC '].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Previously Tx to categotical columnns:\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    if row['Previously Tx'] == 1:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Antibiotics Alone\"\n",
    "    elif row['Previously Tx'] == 2:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Steroids Alone\"\n",
    "    elif row['Previously Tx'] == 3:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Abx Steroids\"\n",
    "    elif row['Previously Tx'] == 4:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Abx + Aspiration attempt\"\n",
    "    elif row['Previously Tx'] == 5:\n",
    "        data.loc[idx, 'Previously Tx'] = \"Pain Meds alone\"\n",
    "    elif row['Previously Tx'] == 0:\n",
    "        data.loc[idx, 'Previously Tx'] = \"no treatment\"\n",
    "        \n",
    "data = create_dummies(data, 'Previously Tx').drop(columns='Previously Tx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1056 entries, 0 to 1056\n",
      "Data columns (total 74 columns):\n",
      "MRN                                       1056 non-null int64\n",
      "Month                                     1055 non-null float64\n",
      "Date                                      1055 non-null datetime64[ns]\n",
      "Age                                       1055 non-null float64\n",
      "Gender                                    1056 non-null int64\n",
      "Duration Sxs (days)                       1055 non-null object\n",
      "Fever                                     1049 non-null float64\n",
      "Sore Throat                               1055 non-null float64\n",
      "Worsening of Symptoms                     1051 non-null float64\n",
      "Otalgia                                   1055 non-null float64\n",
      "Trismus                                   1054 non-null float64\n",
      "Cough                                     1048 non-null float64\n",
      "Voice Change                              1054 non-null float64\n",
      "Dysphagia                                 1055 non-null float64\n",
      "Dyspnea                                   1055 non-null float64\n",
      "Anorexia                                  1055 non-null float64\n",
      "Neck Pain                                 1054 non-null float64\n",
      "LND                                       1052 non-null object\n",
      "Other Sxs                                 141 non-null object\n",
      "Recurrent Event                           41 non-null float64\n",
      "WBC                                       774 non-null float64\n",
      "Antibx                                    1051 non-null object\n",
      "Aspiration Att                            1055 non-null float64\n",
      "Pus                                       916 non-null float64\n",
      "Amount Pus                                898 non-null object\n",
      "Admit Days                                1027 non-null float64\n",
      "Quinsy Tonsillectomy                      804 non-null float64\n",
      "Tonsillectomy                             1043 non-null object\n",
      "Tonsillectomy Date                        205 non-null datetime64[ns]\n",
      "Interval to Tonsillectomy                 205 non-null float64\n",
      "Tonsil Bleed                              212 non-null float64\n",
      "Tonsil Bleed Date                         39 non-null object\n",
      "Tonsil Bleed Tx Method                    27 non-null float64\n",
      "Strep                                     755 non-null object\n",
      "Mono                                      555 non-null object\n",
      "CT                                        1046 non-null float64\n",
      "F/u                                       1052 non-null float64\n",
      "Recurrence                                1055 non-null float64\n",
      "Recurrence Date #1                        158 non-null datetime64[ns]\n",
      "Interval to Rec #1 (days)                 156 non-null float64\n",
      "Recurrence Date #2                        30 non-null datetime64[ns]\n",
      "Recurrence Date #3                        4 non-null datetime64[ns]\n",
      "F. nucleatum                              27 non-null float64\n",
      "F. Necrophorum                            154 non-null float64\n",
      "Strep species                             204 non-null float64\n",
      "Staph species                             20 non-null float64\n",
      "Prevotella                                67 non-null float64\n",
      "Bacteroides                               6 non-null float64\n",
      "Eikenella                                 15 non-null float64\n",
      "H. flu                                    15 non-null float64\n",
      "Other                                     22 non-null object\n",
      "Oral Flora                                2 non-null float64\n",
      "No organisms grown                        79 non-null float64\n",
      "None Sent                                 138 non-null float64\n",
      "Culture Data                              584 non-null object\n",
      "ITA                                       823 non-null float64\n",
      "ITA-R                                     822 non-null float64\n",
      "ITA-C                                     822 non-null float64\n",
      "PTA                                       1022 non-null float64\n",
      "Other Dx                                  338 non-null object\n",
      "Other Notes                               180 non-null object\n",
      "Unnamed: 62                               1 non-null float64\n",
      "Age_cat                                   1047 non-null object\n",
      "Age_cat_50+                               1056 non-null uint8\n",
      "Age_cat_Adult                             1056 non-null uint8\n",
      "Age_cat_Teenager                          1056 non-null uint8\n",
      "Age_cat_Young Adult                       1056 non-null uint8\n",
      "Previously Tx_6.0                         1056 non-null uint8\n",
      "Previously Tx_Abx + Aspiration attempt    1056 non-null uint8\n",
      "Previously Tx_Abx Steroids                1056 non-null uint8\n",
      "Previously Tx_Antibiotics Alone           1056 non-null uint8\n",
      "Previously Tx_Pain Meds alone             1056 non-null uint8\n",
      "Previously Tx_Steroids Alone              1056 non-null uint8\n",
      "Previously Tx_no treatment                1056 non-null uint8\n",
      "dtypes: datetime64[ns](5), float64(42), int64(2), object(14), uint8(11)\n",
      "memory usage: 579.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WBC was excluded because it is not very correlated and not always available pre-aspiration attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 892 entries, 0 to 1056\n",
      "Data columns (total 23 columns):\n",
      "Age                                       892 non-null float64\n",
      "Gender                                    892 non-null int64\n",
      "Duration Sxs (days)                       892 non-null float64\n",
      "Fever                                     892 non-null float64\n",
      "Otalgia                                   892 non-null float64\n",
      "Trismus                                   892 non-null float64\n",
      "Cough                                     892 non-null float64\n",
      "Dysphagia                                 892 non-null float64\n",
      "Anorexia                                  892 non-null float64\n",
      "Worsening of Symptoms                     892 non-null float64\n",
      "Age_cat_50+                               892 non-null uint8\n",
      "Age_cat_Teenager                          892 non-null uint8\n",
      "Age_cat_Adult                             892 non-null uint8\n",
      "Age_cat_Young Adult                       892 non-null uint8\n",
      "Pus                                       892 non-null float64\n",
      "Tonsillectomy                             892 non-null float64\n",
      "Previously Tx_no treatment                892 non-null uint8\n",
      "Previously Tx_Steroids Alone              892 non-null uint8\n",
      "Previously Tx_Pain Meds alone             892 non-null uint8\n",
      "Previously Tx_Abx Steroids                892 non-null uint8\n",
      "Previously Tx_Abx + Aspiration attempt    892 non-null uint8\n",
      "Previously Tx_6.0                         892 non-null uint8\n",
      "Neck Pain                                 892 non-null float64\n",
      "dtypes: float64(12), int64(1), uint8(10)\n",
      "memory usage: 106.3 KB\n"
     ]
    }
   ],
   "source": [
    "#convert to numeric, drop rows with missing data\n",
    "data['Duration Sxs (days)'] = data['Duration Sxs (days)'].astype(\"float64\")\n",
    "data = data[(data['Tonsillectomy'] != \"Previous\") & (data['Tonsillectomy'] != \"-\")]\n",
    "data['Tonsillectomy'] = data['Tonsillectomy'].astype('float64')\n",
    "data = data[['Age','Gender', 'Duration Sxs (days)', 'Fever', 'Otalgia', \n",
    "           'Trismus','Cough', 'Dysphagia', 'Anorexia', 'Worsening of Symptoms', 'Age_cat_50+', \n",
    "            'Age_cat_Teenager', 'Age_cat_Adult', 'Age_cat_Young Adult', 'Pus', 'Tonsillectomy', 'Previously Tx_no treatment', \n",
    "             'Previously Tx_Steroids Alone', 'Previously Tx_Pain Meds alone', 'Previously Tx_Abx Steroids',\n",
    "             'Previously Tx_Abx + Aspiration attempt', 'Previously Tx_6.0', 'Neck Pain']]\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean age = 24.65493273542601, (1.2-88.0)\n",
      "Percentage male = 0.5011210762331838\n",
      "\n",
      "percent successful aspiration: 0.625560538116592\n"
     ]
    }
   ],
   "source": [
    "print('mean age = {}, ({}-{})'.format(np.mean(data['Age']), data['Age'].min(), data['Age'].max()))\n",
    "print(\"Percentage male = {}\".format(data['Gender'].sum()/data['Gender'].shape[0]))\n",
    "print(\"\")\n",
    "print(\"percent successful aspiration: {}\".format(data['Pus'].sum()/data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "892 rows remain after dropping all with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pus                                       1.000000\n",
       "Trismus                                   0.402845\n",
       "Worsening of Symptoms                     0.271783\n",
       "Duration Sxs (days)                       0.118878\n",
       "Otalgia                                   0.079844\n",
       "Previously Tx_Steroids Alone              0.074299\n",
       "Dysphagia                                 0.071082\n",
       "Previously Tx_Abx Steroids                0.053464\n",
       "Age_cat_Young Adult                       0.051062\n",
       "Previously Tx_Abx + Aspiration attempt    0.046287\n",
       "Gender                                    0.038797\n",
       "Previously Tx_6.0                         0.036676\n",
       "Anorexia                                  0.022655\n",
       "Age                                       0.011300\n",
       "Previously Tx_Pain Meds alone             0.006192\n",
       "Age_cat_50+                              -0.002991\n",
       "Age_cat_Adult                            -0.014064\n",
       "Fever                                    -0.026715\n",
       "Age_cat_Teenager                         -0.046934\n",
       "Cough                                    -0.065090\n",
       "Neck Pain                                -0.093250\n",
       "Previously Tx_no treatment               -0.096126\n",
       "Name: Pus, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = data[['Age','Gender', 'Duration Sxs (days)', 'Fever', 'Otalgia', \n",
    "           'Trismus','Cough', 'Dysphagia', 'Anorexia', 'Worsening of Symptoms', 'Age_cat_50+', \n",
    "            'Age_cat_Teenager', 'Age_cat_Adult', 'Age_cat_Young Adult', 'Pus', 'Previously Tx_no treatment', \n",
    "             'Previously Tx_Steroids Alone', 'Previously Tx_Pain Meds alone', 'Previously Tx_Abx Steroids',\n",
    "             'Previously Tx_Abx + Aspiration attempt', 'Previously Tx_6.0', 'Neck Pain']].corr()\n",
    "corr['Pus'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Anorexia</th>\n",
       "      <th>Cough</th>\n",
       "      <th>Duration Sxs (days)</th>\n",
       "      <th>Dysphagia</th>\n",
       "      <th>Fever</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Neck Pain</th>\n",
       "      <th>Otalgia</th>\n",
       "      <th>Previously Tx_no treatment</th>\n",
       "      <th>Trismus</th>\n",
       "      <th>Worsening of Symptoms</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pus</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>24.485629</td>\n",
       "      <td>0.464072</td>\n",
       "      <td>0.068862</td>\n",
       "      <td>5.907186</td>\n",
       "      <td>0.601796</td>\n",
       "      <td>0.452096</td>\n",
       "      <td>0.476048</td>\n",
       "      <td>0.257485</td>\n",
       "      <td>0.538922</td>\n",
       "      <td>0.559880</td>\n",
       "      <td>0.332335</td>\n",
       "      <td>0.589820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>24.756272</td>\n",
       "      <td>0.487455</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>7.137993</td>\n",
       "      <td>0.672043</td>\n",
       "      <td>0.424731</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.179211</td>\n",
       "      <td>0.620072</td>\n",
       "      <td>0.460573</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.835125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age  Anorexia     Cough  Duration Sxs (days)  Dysphagia     Fever  \\\n",
       "Pus                                                                            \n",
       "0.0  24.485629  0.464072  0.068862             5.907186   0.601796  0.452096   \n",
       "1.0  24.756272  0.487455  0.039427             7.137993   0.672043  0.424731   \n",
       "\n",
       "       Gender  Neck Pain   Otalgia  Previously Tx_no treatment   Trismus  \\\n",
       "Pus                                                                        \n",
       "0.0  0.476048   0.257485  0.538922                    0.559880  0.332335   \n",
       "1.0  0.516129   0.179211  0.620072                    0.460573  0.741935   \n",
       "\n",
       "     Worsening of Symptoms  \n",
       "Pus                         \n",
       "0.0               0.589820  \n",
       "1.0               0.835125  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data, index=['Pus'],  values = ['Trismus', 'Worsening of Symptoms', 'Duration Sxs (days)', \n",
    "                                               'Otalgia', 'Dysphagia', 'Gender', 'Anorexia', 'Age', 'Fever',\n",
    "                                               'Cough', 'Neck Pain', 'Previously Tx_no treatment' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    558\n",
       "0.0    334\n",
       "Name: Pus, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data, index='Otalgia', values = 'Pus')\n",
    "data['Pus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trismus looks much more predictive of successful aspiration than otalgia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary for storing performance measures for comparison between models\n",
    "accuracy_dict = {}\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# we include the 6 features most correlated with the presence of pus.\n",
    "features = ['Duration Sxs (days)', 'Otalgia', \n",
    "           'Trismus','Worsening of Symptoms', \n",
    "            'Neck Pain', 'Previously Tx_no treatment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and holdout sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced_subsample', 'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 10, 'random_state': 5}\n",
      "0.719551282051\n"
     ]
    }
   ],
   "source": [
    "# Grid search for parameter optimization\n",
    "# this may take a minute to run...\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestClassifier()\n",
    "grid = GridSearchCV(rf, {\n",
    "            \"n_estimators\": [5, 10, 100],\n",
    "            \"criterion\": ['gini'],\n",
    "            \"max_depth\": [1, 3, 5, 7, 9, 15],\n",
    "            \"max_features\": [\"log2\"],\n",
    "            \"min_samples_leaf\": [1, 3, 5, 8, 10],\n",
    "            \"min_samples_split\": [2, 3, 5, 8, 10],\n",
    "            'random_state': [rand_state],\n",
    "            'class_weight': ['balanced', 'balanced_subsample']\n",
    "        }, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_rf = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7061691542288557(0.02395118092798047) \n",
      " Sensitivity: 0.7357076845030353(0.04099373414847039) \n",
      " Specificity: 0.6578041380440068 (0.05058299407749282)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf_accuracy = []\n",
    "rf_accuracy_train = []\n",
    "rf_ppv = []\n",
    "rf_npv = []\n",
    "rf_sensitivity = []\n",
    "rf_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    best_rf.fit(X_train, y_train)\n",
    "    predictions = best_rf.predict(X_test)\n",
    "    predictions_train = best_rf.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    rf_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    rf_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    rf_ppv.append(ppv)\n",
    "    rf_npv.append(npv)\n",
    "    rf_sensitivity.append(sensitivity)\n",
    "    rf_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['Random Forest'] = np.mean(rf_accuracy)\n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({})\".format(np.mean(rf_accuracy), \n",
    "                                                                               np.std(rf_accuracy), \n",
    "                                                                               np.mean(rf_sensitivity),\n",
    "                                                                               np.std(rf_sensitivity),\n",
    "                                                                               np.mean(rf_specificity),\n",
    "                                                                              np.std(rf_specificity)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the test and training sets are similar, suggesting no overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.711538461538\n",
      "{'class_weight': 'balanced', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(lr, {\"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"], 'class_weight': ['', 'balanced']}, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_lr = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679104477612 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.70      0.64       107\n",
      "        1.0       0.77      0.66      0.71       161\n",
      "\n",
      "avg / total       0.69      0.68      0.68       268\n",
      "\n",
      "0.682765426366\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of LR\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "best_lr.fit(X_train, y_train)\n",
    "predictions = best_lr.predict(X_test)\n",
    "predictions_train = best_lr.predict(X_train)\n",
    "\n",
    "print(accuracy_score(y_test, predictions), \"\\n\\n\", classification_report(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7125746268656716(0.02254786456915147) \n",
      " Sensitivity: 0.7668510071227278(0.03928385333716861) \n",
      " Specificity: 0.6229906638426396 (0.05560317476236639)\n"
     ]
    }
   ],
   "source": [
    "lr_accuracy = []\n",
    "lr_accuracy_train = []\n",
    "lr_ppv = []\n",
    "lr_npv = []\n",
    "lr_sensitivity = []\n",
    "lr_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    best_lr.fit(X_train, y_train)\n",
    "    predictions = best_lr.predict(X_test)\n",
    "    predictions_train = best_lr.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    lr_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    lr_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    lr_ppv.append(ppv)\n",
    "    lr_npv.append(npv)\n",
    "    lr_sensitivity.append(sensitivity)\n",
    "    lr_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['Log regression'] = np.mean(lr_accuracy)\n",
    "    \n",
    "    \n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({})\".format(np.mean(lr_accuracy), \n",
    "                                                                               np.std(lr_accuracy), \n",
    "                                                                               np.mean(lr_sensitivity),\n",
    "                                                                               np.std(lr_sensitivity),\n",
    "                                                                               np.mean(lr_specificity),\n",
    "                                                                              np.std(lr_specificity)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is less accurate than Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.735576923077\n",
      "{'algorithm': 'brute', 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, {\n",
    "            'n_neighbors': range(1,20,2),\n",
    "            'weights': ['distance', 'uniform'],\n",
    "            'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "            'p': [1,2]\n",
    "        }, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_knn = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6999129353233831(0.022548385993197938) \n",
      " Sensitivity: 0.8244571480287384(0.03534493819520581) \n",
      " Specificity: 0.49646696069718377 (0.05468574954254248)\n"
     ]
    }
   ],
   "source": [
    "knn_accuracy = []\n",
    "knn_accuracy_train = []\n",
    "knn_ppv = []\n",
    "knn_npv = []\n",
    "knn_sensitivity = []\n",
    "knn_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    best_knn.fit(X_train, y_train)\n",
    "    predictions = best_knn.predict(X_test)\n",
    "    predictions_train = best_knn.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    knn_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    knn_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    knn_ppv.append(ppv)\n",
    "    knn_npv.append(npv)\n",
    "    knn_sensitivity.append(sensitivity)\n",
    "    knn_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['KNN'] = np.mean(knn_accuracy)    \n",
    "    \n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({})\".format(np.mean(knn_accuracy), \n",
    "                                                                               np.std(knn_accuracy), \n",
    "                                                                               np.mean(knn_sensitivity),\n",
    "                                                                               np.std(knn_sensitivity),\n",
    "                                                                               np.mean(knn_specificity),\n",
    "                                                                              np.std(knn_specificity)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708955223881 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.52      0.59       107\n",
      "        1.0       0.72      0.83      0.77       161\n",
      "\n",
      "avg / total       0.70      0.71      0.70       268\n",
      "\n",
      "0.677831311314\n"
     ]
    }
   ],
   "source": [
    "# evaluate knn performance:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "best_knn.fit(X_train, y_train)\n",
    "predictions = best_knn.predict(X_test)\n",
    "predictions_train = best_knn.predict(X_train)\n",
    "    \n",
    "print(accuracy_score(y_test, predictions) , \"\\n\\n\", classification_report(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731343283582 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.72      0.54      0.62       107\n",
      "        1.0       0.74      0.86      0.79       161\n",
      "\n",
      "avg / total       0.73      0.73      0.72       268\n",
      "\n",
      "0.699599465955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter = 2000, random_state=rand_state)\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy , \"\\n\\n\", classification_report(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.748397435897\n",
      "{'alpha': 1, 'hidden_layer_sizes': [300, 150], 'max_iter': 2000, 'random_state': 5}\n"
     ]
    }
   ],
   "source": [
    "#evaluate optimal parameters:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                    test_size=0.3, random_state=rand_state)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier()\n",
    "grid = GridSearchCV(mlp, {\n",
    "            'alpha': [10, 1, .01, .001],\n",
    "            'max_iter': [2000],\n",
    "            'hidden_layer_sizes': [[300, 150], [300, 100, 50]],\n",
    "            'random_state':[rand_state]}, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_mlp = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731343283582              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      0.50      0.60       107\n",
      "        1.0       0.73      0.89      0.80       161\n",
      "\n",
      "avg / total       0.73      0.73      0.72       268\n",
      "\n",
      "0.691762930284\n"
     ]
    }
   ],
   "source": [
    "#evaluate performance:\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "predictions = best_mlp.predict(X_test)\n",
    "predictions_train = best_mlp.predict(X_train)\n",
    "\n",
    "print(accuracy_score(y_test, predictions), classification_report(y_test, predictions))\n",
    "print(roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7233457711442787(0.02197677429020845) \n",
      " Sensitivity: 0.8654378549875923(0.03567849810642601) \n",
      " Specificity: 0.4910720529688987 (0.06072565707223755)\n"
     ]
    }
   ],
   "source": [
    "mlp_accuracy = []\n",
    "mlp_accuracy_train = []\n",
    "mlp_ppv = []\n",
    "mlp_npv = []\n",
    "mlp_sensitivity = []\n",
    "mlp_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPClassifier(max_iter=2000, alpha=1, hidden_layer_sizes=[300, 150, 100])\n",
    "    best_mlp.fit(X_train, y_train)\n",
    "    predictions = best_mlp.predict(X_test)\n",
    "    predictions_train = best_mlp.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    mlp_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    mlp_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    mlp_ppv.append(ppv)\n",
    "    mlp_npv.append(npv)\n",
    "    mlp_sensitivity.append(sensitivity)\n",
    "    mlp_specificity.append(specificity)    \n",
    "\n",
    "accuracy_dict['MLP'] = np.mean(mlp_accuracy)\n",
    "\n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({})\".format(np.mean(mlp_accuracy), \n",
    "                                                                               np.std(mlp_accuracy), \n",
    "                                                                               np.mean(mlp_sensitivity),\n",
    "                                                                               np.std(mlp_sensitivity),\n",
    "                                                                               np.mean(mlp_specificity),\n",
    "                                                                              np.std(mlp_specificity)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superior to Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7026616915422886(0.021830268091242517) \n",
      " Sensitivity: 0.8143741837055172(0.03579171051552516) \n",
      " Specificity: 0.5205591240577001 (0.053286871529534305)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc_accuracy = []\n",
    "gbc_accuracy_train = []\n",
    "gbc_ppv = []\n",
    "gbc_npv = []\n",
    "gbc_sensitivity = []\n",
    "gbc_specificity = []\n",
    "\n",
    "for i in range (300):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=i)\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    gbc = GradientBoostingClassifier()\n",
    "    gbc.fit(X_train, y_train)\n",
    "    predictions = gbc.predict(X_test)\n",
    "    predictions_train = gbc.predict(X_train)\n",
    "#calculate sensitivity, specificity, ppv, npv\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_test, predictions)\n",
    "    npv = precision[0]\n",
    "    ppv = precision[1]\n",
    "    specificity = recall[0]\n",
    "    sensitivity = recall[1]\n",
    "#append to lists \n",
    "    gbc_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    gbc_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    gbc_ppv.append(ppv)\n",
    "    gbc_npv.append(npv)\n",
    "    gbc_sensitivity.append(sensitivity)\n",
    "    gbc_specificity.append(specificity)    \n",
    "\n",
    "print(\"Accuracy: {}({}) \\n Sensitivity: {}({}) \\n Specificity: {} ({})\".format(np.mean(gbc_accuracy), \n",
    "                                                                               np.std(gbc_accuracy), \n",
    "                                                                               np.mean(gbc_sensitivity),\n",
    "                                                                               np.std(gbc_sensitivity),\n",
    "                                                                               np.mean(gbc_specificity),\n",
    "                                                                              np.std(gbc_specificity)))\n",
    "accuracy_dict['SVM'] = np.mean(gbc_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Random Forest': 0.70616915422885573, 'Log regression': 0.71257462686567163, 'KNN': 0.69991293532338306, 'MLP': 0.72334577114427867, 'SVM': 0.70266169154228864}\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, ENT specialists are only 64% accurate, so this is better than human prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.730769230769\n",
      "{'C': 1, 'class_weight': 'balanced', 'shrinking': True}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is leftover code from other explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "grid = GridSearchCV(svc, {\n",
    "            'C': [100, 10, 1, .01, .001, .0001],\n",
    "            'shrinking': [True, False],\n",
    "            'class_weight':['balanced','']}, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_svc = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.717014925373 0.725625\n",
      "0.652934407365\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "svc_accuracy = []\n",
    "svc_accuracy_train = []\n",
    "for i in range (100):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.3, random_state=i)\n",
    "\n",
    "\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    best_svc.fit(X_train, y_train)\n",
    "    predictions = best_svc.predict(X_test)\n",
    "    predictions_train = best_svc.predict(X_train)\n",
    "    svc_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    svc_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "\n",
    "print(np.mean(svc_accuracy), np.mean(svc_accuracy_train))\n",
    "print(roc_auc_score(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e6b3651a636f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mbest_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_features' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def model_selector(df, features):\n",
    "    \n",
    "    models = [{\n",
    "        \"name\": 'K Neighbors Classifier', \n",
    "        'estimator':KNeighborsClassifier(),\n",
    "        'hyperparameters': {\n",
    "            'n_neighbors': range(1,20,2),\n",
    "            'weights': ['distance', 'uniform'],\n",
    "            'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "            'p': [1,2]\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        \"name\": 'Logistic Regression',\n",
    "        'estimator':LogisticRegression(),\n",
    "        'hyperparameters': {\n",
    "            \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"], \n",
    "        }\n",
    "        \n",
    "    }, \n",
    "    {\n",
    "        'name': \"Random Forest Classifier\",\n",
    "        'estimator':RandomForestClassifier(), \n",
    "        'hyperparameters': {\n",
    "            \"n_estimators\": [10, 50, 200],\n",
    "            \"criterion\": [\"entropy\", \"gini\"],\n",
    "            \"max_depth\": [2, 5, 10],\n",
    "            \"max_features\": [\"log2\", \"sqrt\"],\n",
    "            \"min_samples_leaf\": [1, 5, 8],\n",
    "            \"min_samples_split\": [2, 3, 5]\n",
    "        }\n",
    "    }]\n",
    "    for model in models:\n",
    "        print(model['name'], \"\\n_____________\\n\")\n",
    "        grid = GridSearchCV(model[\"estimator\"], \n",
    "                            param_grid=model[\"hyperparameters\"], \n",
    "                           cv = 10)\n",
    "        grid.fit(X_train, y_train)\n",
    "        model['best model'] = grid.best_estimator_\n",
    "        model['best parameters'] = grid.best_params_\n",
    "        model['best score'] = grid.best_score_\n",
    "        print('best score:',grid.best_score_)\n",
    "        print('best parameters:', grid.best_params_)\n",
    "        print(\"\\n\\n\")\n",
    "    return models\n",
    "best_models = model_selector(data, best_features)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "rf = RandomForestClassifier(max_depth=2, max_features='log2', min_samples_leaf=5, \n",
    "                            min_samples_split=2, n_estimators=200)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "predictions_train = rf.predict(X_train)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions), accuracy_score(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Tonsillectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_tonsillectomy = data.corr()\n",
    "(corr_tonsillectomy['Tonsillectomy']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tonsil_model = model_selector(data, best_features, 'Pus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[best_features], data['Tonsillectomy'], test_size=0.33, random_state=42)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=10, max_features='log2', min_samples_leaf=20, \n",
    "                            min_samples_split=2, n_estimators=4, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "predictions_train = rf.predict(X_train)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "print(accuracy_train, accuracy)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter = 2000)\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pus = data[data['Pus']==1]\n",
    "neg_pus = data[data['Pus']==0]\n",
    "samp = pos_pus.sample(n=neg_pus.shape[0])\n",
    "subsampled_data = pd.concat([neg_pus, samp], axis=0)\n",
    "subsampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
