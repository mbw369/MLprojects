{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "data = pd.read_excel(\"PTA.xlsx\")\n",
    "pd.set_option(\"max_columns\", 9999)\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select variables available pre-aspiration attempt:\n",
    "\n",
    "columns = ['Age', 'Gender', 'Duration Sxs (days)', 'Fever', 'Sore Throat', 'Otalgia', \n",
    "           'Trismus','Cough', 'Dysphagia', 'Anorexia','Worsening of Symptoms', \"Pus\", 'WBC ']\n",
    "\n",
    "# drop the 1 missing age row:\n",
    "\n",
    "data['Age'] = data['Age'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert continuous age variable to categorical:\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    if row['Age'] < 18:\n",
    "        data.loc[idx, 'Age_cat'] = \"Teenager\"\n",
    "    elif row['Age'] >= 18 and row['Age'] < 30:\n",
    "        data.loc[idx, 'Age_cat'] = \"Young Adult\"\n",
    "    elif row['Age'] >= 30 and row['Age'] < 50:\n",
    "        data.loc[idx, 'Age_cat'] = \"Adult\"\n",
    "    elif row['Age'] >50:\n",
    "        data.loc[idx, 'Age_cat'] = \"50+\"\n",
    "\n",
    "def create_dummies(df,column_name):\n",
    "    \"\"\"Create Dummy Columns (One Hot Encoding) from a single Column\n",
    "\n",
    "    Usage\n",
    "    ------\n",
    "    \"\"\"\n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df\n",
    "\n",
    "data = create_dummies(data, 'Age_cat').drop(columns='Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert gender to numerical:\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    if row['Gender'] == \"M\":\n",
    "        data.loc[index,'Gender'] = 1\n",
    "    else:\n",
    "        data.loc[index,'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NR value from Duration of symptoms:\n",
    "\n",
    "data =data [data['Duration Sxs (days)']!= \"NR\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deal with missing values for WBC count:\n",
    "\n",
    "import numpy as np\n",
    "for idx,row in data.iterrows():\n",
    "    if row['WBC '] == 'Not Performed':\n",
    "        data.loc[idx, 'WBC ']=np.nan\n",
    "    elif row['WBC '] == 0:\n",
    "        data.loc[idx, 'WBC ']=np.nan\n",
    "        \n",
    "data['WBC '] = data['WBC '].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 656 entries, 0 to 1055\n",
      "Data columns (total 16 columns):\n",
      "Gender                   656 non-null int64\n",
      "Duration Sxs (days)      656 non-null float64\n",
      "Fever                    656 non-null float64\n",
      "Otalgia                  656 non-null float64\n",
      "Trismus                  656 non-null float64\n",
      "Cough                    656 non-null float64\n",
      "Dysphagia                656 non-null float64\n",
      "Anorexia                 656 non-null float64\n",
      "Worsening of Symptoms    656 non-null float64\n",
      "WBC                      656 non-null float64\n",
      "Age_cat_50+              656 non-null uint8\n",
      "Age_cat_Teenager         656 non-null uint8\n",
      "Age_cat_Adult            656 non-null uint8\n",
      "Age_cat_Young Adult      656 non-null uint8\n",
      "Pus                      656 non-null float64\n",
      "Tonsillectomy            656 non-null float64\n",
      "dtypes: float64(11), int64(1), uint8(4)\n",
      "memory usage: 69.2 KB\n"
     ]
    }
   ],
   "source": [
    "#convert to numeric, drop rows with missing data\n",
    "data['Duration Sxs (days)'] = data['Duration Sxs (days)'].astype(\"float64\")\n",
    "data = data[(data['Tonsillectomy'] != \"Previous\") & (data['Tonsillectomy'] != \"-\")]\n",
    "data['Tonsillectomy'] = data['Tonsillectomy'].astype('float64')\n",
    "data = data[['Gender', 'Duration Sxs (days)', 'Fever', 'Otalgia', \n",
    "           'Trismus','Cough', 'Dysphagia', 'Anorexia', 'Worsening of Symptoms', 'WBC ', 'Age_cat_50+', \n",
    "            'Age_cat_Teenager', 'Age_cat_Adult', 'Age_cat_Young Adult', 'Pus', 'Tonsillectomy']]\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "656 rows remain after dropping all with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pus                      1.000000\n",
       "Trismus                  0.432583\n",
       "Worsening of Symptoms    0.273227\n",
       "Otalgia                  0.109684\n",
       "Duration Sxs (days)      0.102332\n",
       "Age_cat_Young Adult      0.065727\n",
       "Dysphagia                0.061706\n",
       "Gender                   0.056037\n",
       "Anorexia                 0.014675\n",
       "Age_cat_50+             -0.001339\n",
       "Fever                   -0.008247\n",
       "Age_cat_Adult           -0.019917\n",
       "Cough                   -0.053863\n",
       "Age_cat_Teenager        -0.071192\n",
       "Name: Pus, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = data[['Gender', 'Duration Sxs (days)', 'Fever', 'Otalgia', \n",
    "           'Trismus','Cough', 'Dysphagia', 'Anorexia', 'Worsening of Symptoms', 'Age_cat_50+', \n",
    "            'Age_cat_Teenager', 'Age_cat_Adult', 'Age_cat_Young Adult', 'Pus']].corr()\n",
    "corr['Pus'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trismus</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.367041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.794344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pus\n",
       "Trismus          \n",
       "0.0      0.367041\n",
       "1.0      0.794344"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data, index='Trismus', values = 'Pus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pus</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Otalgia</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.557971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.665789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Pus\n",
       "Otalgia          \n",
       "0.0      0.557971\n",
       "1.0      0.665789"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(data, index='Otalgia', values = 'Pus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trismus looks much more predictive of successful aspiration than otalgia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best features \n",
      " _________________ \n",
      " ['Duration Sxs (days)', 'Trismus', 'Worsening of Symptoms']\n"
     ]
    }
   ],
   "source": [
    "# feature selection:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "def RFECV_selection(df, features):\n",
    "    all_X = df[features]\n",
    "    all_y = df['Pus']\n",
    "    rf = RandomForestClassifier(random_state=1)\n",
    "    rfecv = RFECV(rf, cv=10)\n",
    "    rfecv.fit(all_X, all_y)\n",
    "    print(\"best features \\n _________________ \\n\", \n",
    "          list(all_X.columns[rfecv.support_]))\n",
    "    return list(all_X.columns[rfecv.support_])\n",
    "\n",
    "features = ['Gender', 'Duration Sxs (days)', 'Fever', 'Otalgia', \n",
    "           'Trismus','Cough', 'Dysphagia', 'Anorexia', 'Worsening of Symptoms', 'Age_cat_50+', \n",
    "            'Age_cat_Teenager', 'Age_cat_Adult', 'Age_cat_Young Adult']\n",
    "\n",
    "best_features = RFECV_selection(data, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, eliminates all but duration of symptoms, trismus, and worsening of symptoms. However, in the interest of interpretability and broader applicability (beyond just patients already selected for specialist evaluation of PTA), we opted to include all features in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model:\n",
    "\n",
    "# create training and holdout sets:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], test_size=0.3, random_state=1)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 15, 'min_samples_split': 5, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Grid search for parameter optimization\n",
    "# this may take a minute to run...\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rf = RandomForestClassifier()\n",
    "grid = GridSearchCV(rf, {\n",
    "            \"n_estimators\": [10, 50, 100],\n",
    "            \"criterion\": [\"entropy\", \"gini\"],\n",
    "            \"max_depth\": [2, 5, 10, 20],\n",
    "            \"max_features\": [\"log2\"],\n",
    "            \"min_samples_leaf\": [1, 5, 8, 15],\n",
    "            \"min_samples_split\": [2, 3, 5, 9, 15]\n",
    "        }, cv = 10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_rf = grid.best_estimator_\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761421319797 0.762527233115\n",
      "(array([ 0.68627451,  0.78767123]), array([ 0.53030303,  0.8778626 ]), array([ 0.5982906 ,  0.83032491]), array([ 66, 131]))\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.53      0.60        66\n",
      "        1.0       0.79      0.88      0.83       131\n",
      "\n",
      "avg / total       0.75      0.76      0.75       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "predictions = best_rf.predict(X_test)\n",
    "predictions_train = best_rf.predict(X_train)\n",
    "accuracy_test = accuracy_score(y_test, predictions)\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "print(accuracy_test, accuracy_train)\n",
    "print(precision_recall_fscore_support(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the test and training sets are similar, suggesting no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726192893401\n",
      "0.780091503268\n"
     ]
    }
   ],
   "source": [
    "#randomly divide the data into training and test sets 100 times, train and test the model, and average the accuracy\n",
    "rf_accuracy = []\n",
    "rf_accuracy_train = []\n",
    "\n",
    "for i in range (500):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    \n",
    "    best_rf.fit(X_train, y_train)\n",
    "    predictions = best_rf.predict(X_test)\n",
    "    predictions_train = best_rf.predict(X_train)\n",
    "    rf_accuracy.append(accuracy_score(y_test, predictions))\n",
    "    rf_accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    \n",
    "    \n",
    "print(np.mean(rf_accuracy))\n",
    "print(np.mean(rf_accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692810457516\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(lr, {\"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"]}, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_lr = grid.best_estimator_\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.690355329949\n",
      "0.692549019608\n"
     ]
    }
   ],
   "source": [
    "#randomly divide the data into training and test sets 100 times, train and test the model, and average the accuracy\n",
    "accuracy = []\n",
    "accuracy_train = []\n",
    "for i in range (100):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30, random_state=np.random.randint(1, 100))\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "    \n",
    "    best_lr.fit(X_train, y_train)\n",
    "    predictions = best_lr.predict(X_test)\n",
    "    predictions_train = best_lr.predict(X_train)\n",
    "    accuracy.append(accuracy_score(y_test, predictions))\n",
    "    accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "print(np.mean(accuracy))\n",
    "print(np.mean(accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is less accurate than Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.705882352941\n",
      "{'algorithm': 'brute', 'n_neighbors': 13, 'p': 1, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid = GridSearchCV(knn, {\n",
    "            'n_neighbors': range(1,20,2),\n",
    "            'weights': ['distance', 'uniform'],\n",
    "            'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "            'p': [1,2]\n",
    "        }, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_knn = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741116751269 0.723311546841\n"
     ]
    }
   ],
   "source": [
    "#evaluate performance:\n",
    "\n",
    "best_knn.fit(X_train, y_train)\n",
    "predictions = best_knn.predict(X_test)\n",
    "predictions_train = best_knn.predict(X_train)\n",
    "accuracy_test = accuracy_score(y_test, predictions)\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "print(accuracy_test, accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699492385787\n",
      "0.686710239651\n"
     ]
    }
   ],
   "source": [
    "#randomly divide the data into training and test sets 100 times, train and test the model, and average the accuracy\n",
    "accuracy = []\n",
    "accuracy_train = []\n",
    "for i in range (100):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[features], data['Pus'], \n",
    "                                                        test_size=0.30)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    best_knn.fit(X_train, y_train)\n",
    "    predictions = best_lr.predict(X_test)\n",
    "    predictions_train = best_lr.predict(X_train)\n",
    "    accuracy.append(accuracy_score(y_test, predictions))\n",
    "    accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    \n",
    "print(np.mean(accuracy))\n",
    "print(np.mean(accuracy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than LR, but still not as good as random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.771573604061 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.56      0.62        66\n",
      "        1.0       0.80      0.88      0.84       131\n",
      "\n",
      "avg / total       0.76      0.77      0.76       197\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter = 2000)\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy, \"\\n\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727668845316\n",
      "{'alpha': 1, 'hidden_layer_sizes': [300, 100, 50], 'max_iter': 2000}\n"
     ]
    }
   ],
   "source": [
    "#evaluate optimal parameters:\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier()\n",
    "grid = GridSearchCV(mlp, {\n",
    "            'alpha': [100, 10, 1, .01, .001, .0001],\n",
    "            'max_iter': [2000],\n",
    "            'hidden_layer_sizes': [[150, 150, 150, 150, 150], [300, 150], [300, 100, 50], [100, 100, 100, 100]],\n",
    "            }, cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "best_mlp = grid.best_estimator_\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.739949238579 0.747189542484\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "accuracy_train = []\n",
    "for i in range (100):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[best_features], data['Pus'], \n",
    "                                                        test_size=0.30)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    best_mlp.fit(X_train, y_train)\n",
    "    predictions = best_mlp.predict(X_test)\n",
    "    predictions_train = best_mlp.predict(X_train)\n",
    "    accuracy.append(accuracy_score(y_test, predictions))\n",
    "    accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "\n",
    "print(np.mean(accuracy), np.mean(accuracy_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Superior to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is leftover code from other explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Neighbors Classifier \n",
      "_____________\n",
      "\n",
      "best score: 0.758169934641\n",
      "best parameters: {'algorithm': 'ball_tree', 'n_neighbors': 11, 'p': 1, 'weights': 'distance'}\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression \n",
      "_____________\n",
      "\n",
      "best score: 0.751633986928\n",
      "best parameters: {'solver': 'newton-cg'}\n",
      "\n",
      "\n",
      "\n",
      "Random Forest Classifier \n",
      "_____________\n",
      "\n",
      "best score: 0.762527233115\n",
      "best parameters: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 3, 'n_estimators': 10}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def model_selector(df, features):\n",
    "    \n",
    "    models = [{\n",
    "        \"name\": 'K Neighbors Classifier', \n",
    "        'estimator':KNeighborsClassifier(),\n",
    "        'hyperparameters': {\n",
    "            'n_neighbors': range(1,20,2),\n",
    "            'weights': ['distance', 'uniform'],\n",
    "            'algorithm': ['ball_tree', 'kd_tree', 'brute'],\n",
    "            'p': [1,2]\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        \"name\": 'Logistic Regression',\n",
    "        'estimator':LogisticRegression(),\n",
    "        'hyperparameters': {\n",
    "            \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"], \n",
    "        }\n",
    "        \n",
    "    }, \n",
    "    {\n",
    "        'name': \"Random Forest Classifier\",\n",
    "        'estimator':RandomForestClassifier(), \n",
    "        'hyperparameters': {\n",
    "            \"n_estimators\": [10, 50, 200],\n",
    "            \"criterion\": [\"entropy\", \"gini\"],\n",
    "            \"max_depth\": [2, 5, 10],\n",
    "            \"max_features\": [\"log2\", \"sqrt\"],\n",
    "            \"min_samples_leaf\": [1, 5, 8],\n",
    "            \"min_samples_split\": [2, 3, 5]\n",
    "        }\n",
    "    }]\n",
    "    for model in models:\n",
    "        print(model['name'], \"\\n_____________\\n\")\n",
    "        grid = GridSearchCV(model[\"estimator\"], \n",
    "                            param_grid=model[\"hyperparameters\"], \n",
    "                           cv = 10)\n",
    "        grid.fit(X_train, y_train)\n",
    "        model['best model'] = grid.best_estimator_\n",
    "        model['best parameters'] = grid.best_params_\n",
    "        model['best score'] = grid.best_score_\n",
    "        print('best score:',grid.best_score_)\n",
    "        print('best parameters:', grid.best_params_)\n",
    "        print(\"\\n\\n\")\n",
    "    return models\n",
    "best_models = model_selector(data, best_features)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      0.36      0.50        74\n",
      "        1.0       0.71      0.95      0.82       123\n",
      "\n",
      "avg / total       0.75      0.73      0.70       197\n",
      "\n",
      "0.730964467005 0.738562091503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "rf = RandomForestClassifier(max_depth=2, max_features='log2', min_samples_leaf=5, \n",
    "                            min_samples_split=2, n_estimators=200)\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "predictions_train = rf.predict(X_train)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions), accuracy_score(y_train, predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Tonsillectomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tonsillectomy            1.000000\n",
       "Age_cat_Teenager         0.201513\n",
       "Worsening of Symptoms    0.092711\n",
       "Dysphagia                0.059006\n",
       "Pus                      0.049337\n",
       "Duration Sxs (days)      0.041409\n",
       "Anorexia                 0.023301\n",
       "WBC                      0.016059\n",
       "Trismus                  0.013276\n",
       "Fever                    0.008162\n",
       "Otalgia                 -0.004432\n",
       "Cough                   -0.036175\n",
       "Gender                  -0.048185\n",
       "Age_cat_Young Adult     -0.070460\n",
       "Age_cat_Adult           -0.072168\n",
       "Age_cat_50+             -0.097153\n",
       "Name: Tonsillectomy, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_tonsillectomy = data.corr()\n",
    "(corr_tonsillectomy['Tonsillectomy']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model_selector() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-82ff9ccf624b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_tonsil_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Pus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: model_selector() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "best_tonsil_model = model_selector(data, best_features, 'Pus')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[best_features], data['Tonsillectomy'], test_size=0.33, random_state=42)\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', max_depth=10, max_features='log2', min_samples_leaf=20, \n",
    "                            min_samples_split=2, n_estimators=4, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "predictions = rf.predict(X_test)\n",
    "predictions_train = rf.predict(X_train)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "print(accuracy_train, accuracy)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "mlp = MLPClassifier(max_iter = 2000)\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "accuracy_train = []\n",
    "for i in range (150):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[best_features], data['Pus'], \n",
    "                                                        test_size=0.20, random_state=i)\n",
    "    X_train = normalize(X_train)\n",
    "    X_test = normalize(X_test)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    mlp = MLPClassifier(max_iter = 2000, alpha=3)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    predictions = mlp.predict(X_test)\n",
    "    predictions_train = mlp.predict(X_train)\n",
    "    accuracy.append(accuracy_score(y_test, predictions))\n",
    "    accuracy_train.append(accuracy_score(y_train, predictions_train))\n",
    "    #print(classification_report(y_test, predictions))\n",
    "\n",
    "print(np.mean(accuracy), np.mean(accuracy_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
